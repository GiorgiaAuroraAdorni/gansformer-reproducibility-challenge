\PassOptionsToPackage{square,comma,numbers,sort&compress}{natbib}

\documentclass{article}

\usepackage[preprint]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}      % use 8-bit T1 fonts
\usepackage{hyperref}      		 % hyperlinks
\usepackage{url}            			% simple URL typesetting
\usepackage{booktabs}     		 % professional-quality tables
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{nicefrac}      		  % compact symbols for 1/2, etc.
\usepackage{microtype}     		% microtypography
\usepackage[inline]{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}       	 % blackboard math symbols
\usepackage{bm}
\usepackage{mathtools} 

\title{ATML Report}

\author{%
		Giorgia Adorni \\
		\texttt{giorgia.adorni@usi.ch} \\
		\And
		Felix Boelter\\
		\texttt{felix.boelter@usi.ch}\\
		\And
		Stefano Carlo Lambertenghi\\
		\texttt{stefano.carlo.lambertenghi@usi.ch}\\
}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		%Train a GAN with Transformer-like architecture
		%Concise and self-contained description of your project, motivation and main findings.
	\end{abstract}
	
	%	\section*{General notes}
	%	
	%	The report should be written as an article intended to present the findings of your work. Your 
	%	aim should be to be clear and objective, substantiating your claims with references or 
	%	empirical/theoretical evidence.
	%	
	%	We are well aware of the fact that carrying out machine learning experiments might be difficult 
	%	and that often the final performance might be disappointing. For this reason, you will not be 
	%	evaluated solely on quantitative aspect of your work, but mainly on the quality of your analysis 
	%	and report.
	%	
	%	The length of the report should be between 4 and 8 pages (without considering references).
	
	\section{Introduction}
	This objective of this project is to investigate the reliability and reproducibility of a paper accepted 
	for publication in a top machine learning conference.
	The models have been implemented using code and information provided by the authors.	
	Actually we are not participating formally to the \textit{Fall Edition of the ML Reproducibility 
	Challenge 2021}.
	
	With this work, we are going to verify the empirical results and claims of the paper 
	``\emph{Generative adversarial transformers}'' by \citet{hudson2021generative}, by reproducing 
	three of the computational experiments performed by the authors:
	\begin{enumerate*}
		\item[(1)] the \textbf{StyleGAN2} by \citet{karras2020analyzing} \cite{karras2019style}, a GAN 
		network which uses one 
		global latent style vector to modulate the features of each layer, hence to control the style of all 
		image features globally,
		\item[(2)] the \textbf{GANformer} with \textbf{Simplex Attention} by 
		\citet{hudson2021generative}, a generalisation of the StyleGAN design with \textit{k} latent 
		vectors that cooperate through attention, allowing for a spatially finer control over the generation 
		process since multiple style vectors impact different regions in the image concurrently, in 
		particular permitting communication in one direction, in the generative context – from the latents 
		to the image features, and
		\item[(3)] the \textbf{GANformer} with \textbf{Duplex Attention} by \citet{hudson2021generative}, 
		which is based on the same principles as the previous but propagating information both from 
		global latents to local image features, enabling both top-down and bottom-up reasoning to occur 
		simultaneously.
	\end{enumerate*} 
	
	The first model is used as baseline, while the remaining are the architectures introduced by the 
	authors. Thy consider the GANformer has ``a novel and efficient type of transformer'' which 
	demonstrates its strength and robustness over a range of task of visual generative modelling —  
	simulated multi-object environments (real-world indoor and out-door scenes) — achieving 
	state-of-the-art results in terms of both image quality and diversity, while benefiting from fast 
	learning and better data-efficiency. 
	
	% summary of the main results and contributions

	\section{Related works/Background}	
	
	\subsection{Generative Adversarial Networks (GANs)}\label{sec:gan}
	Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, are discriminative models 
	that learn to determine whether a sample is from the model distribution or the data distribution. 
	
	The architecture is composed by two main neural networks: a \textbf{generator} $\bm{G(z)}$, 
	which captures the data distribution mapping random noise $z$ to the data space, and a 
	\textbf{discriminator} $\bm{D(x)}$ which estimates the probability that sample $x$ is generated by 
	$G$ or is a real sample, so it is coming from the training data.

	The training procedure has two objective: the discriminator $D$ is trained to maximise the 
	probability of assigning the correct label to both training examples and samples from $G$, 
	while the generator $G$ is trained to maximise the probability of $D$ making a mistake, so it aims to 
	minimise $\log(1-D(G(z))))$.

	Combining the two objectives for $G(z)$ and $D(x)$ we get the \textit{GAN min-max game} with the 
	value 
	function $V(G,D)$:
	
	\begin{equation}
	\label{e:minmaxgame}
	\min_G \max_D V(G,D) = 
	\mathbb{E}_{x \sim p*(x)} [\log D(x)] + \mathbb E _{z \sim p_z(z)} [\log (1-D(G(z))))]
	\end{equation}
	
	\subsection{StyleGAN2}%TODO fixme
	StyleGAN are a re-design of the GANs generator architecture, aimed at controlling the image 
	synthesis process \cite{karras2019style}.
	
	As mentioned by \citet{karras2020analyzing}, <<this generator starts from a learned constant 
	input and adjusts the ``style'' of the image at each convolution layer based on the latent code, 
	therefore directly controlling the strength of image features at different scales. 
	Combined with noise injected directly into the network, this architectural change leads to automatic, 
	unsupervised separation of high-level attributes (e.g., pose, identity) from stochastic variation (e.g., 
	freckles, hair) in the generated images, and enables intuitive scale-specific mixing and interpolation 
	operations>>.
	
	The generator embeds the input latent code into an intermediate latent space. In particular, the input 
	latent space must follow the probability density of the training data, while the intermediate latent 
	space is free from that restriction and is therefore allowed to be disentangled. 

	Traditionally, the latent code is provided to the generator through an input layer of a feed-forward 
	network. \citet{karras2020analyzing} depart from this design by omitting the input layer altogether 
	and starting from a learned constant instead. 
	Given a latent code $z$ in the input latent space $Z$, a non-linear mapping network $f : Z 
	\rightarrow W$ first produces $w \in W $.
	Learned affine transformations then specialize $w$ to styles $y = (y_s, y_b)$ that control adaptive 
	instance normalization (AdaIN) \cite{?} operations after each convolution layer of the synthesis 
	network $g$. The AdaIN operation is defined as
	\begin{equation}
		\label{e:adain}
		\mathsf{AdaIN}(x_i, y) = y_{s, i} \frac{x_i - \mu(x_i)}{\sigma (x_i)} + y_{b,i}
	\end{equation}
	where each feature map $x_i$ is normalized separately, and then scaled and biased using the 
	corresponding scalar components from style $y$. Thus the dimensionality of $y$ is twice the 
	number of feature maps on that layer.
	
	Moreover, they provide the generator with a direct means to generate stochastic detail by 
	introducing explicit noise inputs. 

	The StyleGAN2 architecture makes it possible to control the image synthesis via scale-specific 
	modifications to the styles. 
	
	\subsection{Transformers}%TODO fixme
	Transformers are architectures which avoid recurrence and instead rely entirely on an attention 
	mechanism to draw global dependencies between input and output	\cite{vaswani2017attention}.
	
	These models rely entirely on \textbf{self-attention} to compute representations of its input and 
	output without using \textit{sequence-aligned RNNs} or \textit{convolution}. 

	The architecture is composed by an encoder-decoder structure where, the \textbf{encoder} maps an 
	input sequence of symbol representations $(x_1,\dots, x_n)$ to a sequence of continuous 
	representations $z = (z_1, \dots, z_n)$, while the  \textbf{decoder}, given $z$, generates an 
	output sequence $(y_1, \dots, y_m)$ of symbols one element at a time. 
	
	At each step the model is auto-regressive, consuming the previously generated symbols as 
	additional input when generating the next.
	In addition to this architecture, the transformer employs \textbf{stacked self-attention} and 
	\textbf{point-wise}, fully connected layers for both the encoder and decoder.
	
	% modules of  N = 6 identical layers, each layer has two sub-layers (1) multi-head self-attention 
	%mechanism and (2) position-wise fully connected feed-forward neural network
	In particular, each transformer module (encoder and decoder) contains $n=6$ identical layers, each 
	with two sub-layers:
		\begin{enumerate*}
			\item[(1)] a \textit{multi-head self-attention mechanism} working in parallel, and 
			\item[(2)] a \textit{position-wise fully connected feed-forward neural network}, with residual 
			connections followed by normalisation.
	\end{enumerate*}
	The decoder self-attention sub-layer has a mask which prevents positions from attending to 
	subsequent positions (so positional encoding provided as additional input to bottom layer). This, 
	combined with fact that the output embeddings are offset by one position, ensures that the 
	predictions for position $i$ can depend only on the known outputs at positions less than $i$.
 	In addition, the decoder has a third sub-layer, which performs \textit{multi-head attention} over the 
 	output of the encoder stack. 
  
	
%	Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two
%	sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, 
%	position-wise fully connected feed-forward network. We employ a residual connection [10] around 
%	each of the two sub-layers, followed by layer normalization [1]. That is, the output of each 
%	sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the 
%	sub-layer itself. 
%	To facilitate these residual connections, all sub-layers in the model, as well as the embedding 
%	layers, produce outputs of dimension dmodel = 512.
%	
%	Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two 
%	sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head 
%	attention over the output of the encoder stack. Similar to the encoder, we employ residual 
%	connections around each of the sub-layers, followed by layer normalization. We also modify the 
%	self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent 
%	positions. This masking, combined with fact that the output embeddings are offset by one 
%	position, ensures that the predictions for position i can depend only on the known outputs at 
%	positions less than i.
	
	An \textit{attention function} \cite{vaswani2017attention}, can be described as mapping a query and 
	a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The 
	output is computed as a weighted sum of the values, where the weight assigned to each value is 
	computed by a compatibility function of the query with the corresponding key.

	The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We 
	compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$ and apply a 
	softmax function to obtain the weights on the values.
	
	In practice, we compute the attention function on a set of queries simultaneously, packed together 
	into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We 
	compute the matrix of outputs as:
	\begin{equation}
		\label{eqn:attention}
		\mathsf{Attention}(Q, K, V) = \mathsf{softmax} \big( \frac{QK^T}{\sqrt{d_k}}\big)V
	\end{equation}
	This is a \textbf{dot-product attention} with a scaling factor of $\sqrt{1}$. 
	
	Instead of performing a single attention function with $d_\text{model}$-dimensional keys, values 
	and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with 
	different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. On each of 
	these projected versions of queries, keys and values we then perform the attention function in 
	parallel, yielding $d_v$-dimensional output values. 
	These are concatenated and once again projected, resulting in the final values.
	\textbf{Multi-head attention} allows the model to jointly attend to information from different 
	representation subspaces at different positions. With a single attention head, averaging inhibits this.
		\begin{equation}
		\label{eqn:multihead}
		\begin{split}
		\mathsf{MultiHead}(Q, K, V) & = \mathsf{Concat}(\mathsf{head}_1 \dots, \mathsf{head}_h) W^O 
		\\
		\text{where} \quad \mathsf{head}_i & = \mathsf{Attention}(QW_i^Q, KW_i^K , VW_i^V)
		\end{split}
	\end{equation}
	
	Where the projections are parameter matrices $W_i^Q \in \mathbb{R}^{d_{\text{model}}\times 
	d_k}$, %TODO $W_i^Q \in \mathbb{R}^{d_{\text{model}}\times d_q}$,  probabile errore
	$W_i^K \in \mathbb{R}^{d_{\text{model}}\times d_k}$, $W_i^V \in 
	\mathbb{R}^{d_{\text{model}}\times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times 
	d_{\text{model}}}$.
	In this work we employ h = 8 parallel attention layers, or heads. For each of these we use $d_k = d_v 
	= d_\text{model}/h = 64$. Due to the reduced dimension of each head, the total 
	computational cost is similar to that of single-head attention with full dimensionality.
	
	The transformer uses multi-head attention in three different ways:
	\begin{enumerate}
		\item In "encoder-decoder attention" layers, the queries come from the previous decoder layer, 
		and the memory keys and values come from the output of the encoder. This allows every position 
		in the decoder to attend over all positions in the input sequence.
		\item The encoder contains self-attention layers, in which all of the keys, values 
		and queries come from the output of the previous layer in the encoder. 
		Each position in the encoder can attend to all positions in the previous layer of the encoder.
		\item Similarly, self-attention layers in the decoder allow each position in the decoder to attend to 
		all positions in the decoder up to and including that position. We need to prevent leftward 
		information flow in the decoder to preserve the auto-regressive property. We implement this 
		inside of scaled dot-product attention by masking out (setting to $-\infty$) all values in the input 
		of the softmax which correspond to illegal connections. 
	\end{enumerate}

	
	\section{Methodology}
	%	In this section you should give a description of the methodological aspects of your work, for 
	%	instance how you modified an existing method to perform a particular task or to overcome a 
	%	particular limitation. If your project is about reproducibility, here you should describe the method 
	%	presented in the original paper.
	
	\subsection{Generative Adversarial Transformers}%TODO fixme	
	The Generative Adversarial Transformer (GANformer), as presented in \cite{hudson2021generative}, 
	is a type of GAN, which involves a \textbf{generator network} ($\bm{G}$), that maps a sample from 
	the latent space to the output space (e.g. an image), and a \textbf{discriminator network}
	($\bm{D}$) which seeks to discern between real and fake samples. As shown in Section 
	\ref{sec:gan}, the two networks compete with each other through a minimax game until reaching an 
	equilibrium. 
	
	The main difference between the GANformer and a GAN is that instead of using multiple layers of 
	convolution they construct them using a \textbf{bipartite transformer}, a structure for computing 
	soft attention, that iteratively aggregates and disseminates information between the generated 
	image features and a compact set of latent variables that function as a bottleneck, to enable 
	bidirectional interaction between these dual representations. 
	
	The \textbf{transformer network} corresponds to the \textit{multi-layer bidirectional transformer 
	encoder} described by \citet{devlin2019bert}, which interleaves \textit{multi-head self-attention} and 
	\textit{feed-forward layers}.
	Each pair of self-attention and feed-forward layers is intended as a \textit{transformer layer}, hence, 
	a transformer is a stack of several such layers. 
	The \textit{self-attention layer} considers all pairwise relations among the input elements, so to 
	update each single element by attending to all the others. 
	The \textbf{bipartite transformer} generalises this formulation, featuring instead a bipartite graph 
	between two groups of variables — in the GAN case, latents and image features. 
	There are two attention operations that could be computed over the bipartite graph, depending on 
	the direction in which information propagates, 
	\begin{enumerate*}
		\item [(1)] \textbf{simplex attention} permits communication either in one way only, in the 
		generative context, from the latents to the image features, and
		\item [(2)] \textbf{duplex attention} which enables it both top-down and bottom-up ways.
	\end{enumerate*}
	
	\subsubsection{Simplex attention}%TODO fixme
	As already mentioned, simplex attention distributes information in a single direction over the 
	bipartite transformer graph. 
	
	Formally, let $X^{n\times d}$ denote an input set of $n$ vectors of dimension $d$ (where, for the 
	image case, $n = W\times H$), and $Y^{m\times d}$ denote a set of $m$ aggregator variables (the 
	latents, in the generative case). We can then compute attention over the derived bipartite graph 
	between these two groups of elements. 
	
	Specifically, we define the attention as in Equation \eqref{eqn:attention}, moreover:
	\begin{equation}
		\label{eqn:attention2}
		a(X,Y)=\mathsf{Attention}(q(X), k(Y), v(Y))
	\end{equation}
	where $q(\cdot), k(\cdot), v(\cdot)$ are functions that respectively map elements into queries, 
	keys, and values, all maintaining dimensionality $d$. We also provide the mappings with positional 
	encodings, to reflect the distinct position of each element (e.g. in the image). Note that this bipartite 
	attention is a generalization of self-attention, where $Y = X$.
	
	We can then integrate the attended information with the input elements $X$, but whereas the 
	standard transformer implements an additive update rule of the form:
	\begin{equation}
		\label{eqn:layernorm}
		u^a(X, Y )=\mathsf{LayerNorm}(X + a(X, Y ))
	\end{equation}
	
	We instead use the retrieved information to control both the scale as well as the bias of the 
	elements 
	in $X$, in line with the practice promoted by the StyleGAN model \cite{karras2019style}. As our 
	experiments indicate, such multiplicative integration enables significant gains in the model 
	performance. 
	
	Formally:
	\begin{equation}
		\label{eqn:simplex}
		u^s(X, Y )=\gamma (a(X, Y )) \odot \omega (X) + \beta (a(X, Y ))
	\end{equation}

	Where $\gamma(\cdot), \beta(\cdot)$ are mappings that compute multiplicative and additive styles 
	(gain and bias), maintaining a dimension of $d$, and $\omega (X) = X- \mu(X)$ normalizes each 
	element with $\sigma(X)$ respect to the other features. By normalizing $X$ (image features), and 
	then letting $Y$ (latents) control the statistical tendencies of $X$, we essentially enable information 
	propagation from $Y$ to $X$, intuitively, allowing the latents to control the visual generation of 
	spatial attended regions within the image, so as to guide the synthesis of objects or entities.
	
	\subsubsection{Duplex attention}%TODO fixme
	To explain duplex attention, lets consider the variables $Y$ to poses a key-value structure of their 
	own: $Y = (K^{n\times d} , V^{n\times d})$, where the values store the content of the $Y$ variables, 
	as before (e.g. the randomly sampled latent vectors in the case of GANs) while the keys track the 
	centroids $K$ of the attention-based assignments between $Y$ and $X$, which can be computed 
	as $K = a(Y, X)$ — namely, the weighted averages of the $X$ elements using the bipartite attention 
	distribution derived through comparing it to $Y$. 
	
	Consequently, we can define a new update rule:
	\begin{equation}
		\label{eqn:duplex}
		u^d(X, Y )=\gamma (A(X, K, V)) \odot \omega (X) + \beta (A(X, K, V))
	\end{equation}

	This update compounds two attention operations on top of each other: where we first (1) compute 
	soft attention assignments between $X$ and $Y$, by $K = a(Y, X)$, and then (2) refine the 
	assignments by considering their centroids, by $A(X, K, V)$. This is analogous to the k-means 
	algorithm and works more effectively than the simpler update $u^a$ defined above in Equation 
	\eqref{eqn:simplex}.
	
	Finally, to support bidirectional interaction between $X$ and $Y$ (the image and the latents), we can 
	chain two reciprocal simplex attentions from $X$ to $Y$ and from $Y$ to $X$, obtaining the duplex 
	attention, which alternates computing $Y :=u^a(Y,X)$ and $X:=u^d(X,Y)$, such that each 
	representation is refined in light of its interaction with the other, integrating together bottom-up and 
	top-down interactions.
	
	\subsubsection{Vision-specific adaptations}%TODO fixme
	A \textit{kernel size} of $k = 3$ is used after each application of the attention, together with a 
	\textit{Leaky ReLU non-linearity} after each convolution and then upsample or downsample the 
	features $X$, as part of the generator or discriminator respectively, as in e.g. StyleGAN2 
	\cite{karras2020analyzing}. 
	To account for the features location within the image, we use a sinusoidal positional encoding along 
	the horizontal and vertical dimensions for the visual features $X$, and trained positional 
	embeddings for the set of latent variables $Y$.
	Overall, the bipartite transformer is thus composed of a stack that alternates attention (simplex or 
	duplex), convolution, and upsampling layers, starting from a $4 \times 4$ grid up to the desirable 
	resolution. 
	
	Conceptually, this structure fosters an interesting communication flow: rather than densely 
	modelling interactions among all the pairs of pixels in the images, it supports adaptive long-range 
	interaction between far away pixels in a moderated manner, passing through a compact and global 
	latent bottleneck that selectively gathers information from the entire input and distributes it back to 
	the relevant regions. Intuitively, this form can be viewed as analogous to the top-down / bottom-up 
	notions discussed in section 1, as information is propagated in the two directions, both from the 
	local pixel to the global high-level representation and vice versa.
	
	Both the simplex and the duplex attention operations enjoy a bilinear efficiency of 
	$\mathcal{O}(mn)$ thanks to the network’s bipartite structure that considers all pairs of 
	corresponding elements from $X$ and $Y$. Since, as we see below, we maintain $Y$ to be of a fairly 
	small size, choosing m in the range of 8–32, this compares favourably to the prohibitive 	
	$\mathcal{O}(n^2)$  complexity of self-attention, which impedes its applicability to high-resolution 
	images.
	
	\subsubsection{The Generator and Discriminator Networks}
	We use the celebrated StyleGAN model as a starting point for our GAN design. Commonly, a 
	generator network consists of a multi-layer CNN that receives a randomly sampled vector z and 
	transforms it into an image. The StyleGAN approach departs from this design and, instead, 
	introduces a feed-forward mapping network that outputs an intermediate vector w, which in turn 
	interacts directly with each convolution through the synthesis network, globally controlling the 
	feature maps’ statistics at every layer.
	Effectively, this approach attains layer-wise decomposition of visual properties, allowing StyleGAN 
	to control global aspects of the picture such as pose, lighting conditions or colour schemes, in a 
	coherent manner over the entire image. But while StyleGAN successfully disentangles global 
	properties, it is more limited in its ability to perform spatial decomposition, as it provides no direct 
	means to control the style of a localized regions within the generated image.
	Luckily, the bipartite transformer offers a solution to meet this goal. Instead of controlling the style 
	of all features globally, we use instead our new attention layer to perform adaptive region-wise 
	modulation. We split the latent vector $z$ into $k$ components, $z = [z_1 ,  
	\dots, z_k ]$ and, as in StyleGAN, pass each of them through a shared mapping network, obtaining 
	a corresponding set of intermediate latent variables $Y = [y_1 , ..., y_k ]$. Then, during synthesis, 
	after each CNN layer in the generator, we let the feature map $X$ and latents $Y$ play the roles of 
	the two element groups, mediating their interaction through our new attention layer (either simplex 
	or duplex). This setting thus allows for a flexible and dynamic style modulation at the region level. 
	Since soft attention tends to group elements based on their proximity and content similarity, we see 
	how the transformer architecture naturally fits into the generative task and proves useful in the 
	visual domain, allowing the model to exercise finer control in modulating local semantic regions. This 
	capability turns to be especially useful in modelling highly-structured scenes.
	For the discriminator, we similarly apply attention after every convolution, in this case using trained 
	embeddings to initialize the aggregator variables $Y$ , which may intuitively represent background 
	knowledge the model learns about the task. At the last layer, we concatenate these variables $Y$ to 
	the final feature map $X$ to make a prediction about the identity of the image source. We note that 
	this construction holds some resemblance to the PatchGAN discriminator, but 
	whereas PatchGAN pools features according to a fixed predetermined scheme, the GANformer can 
	gather the information in a more adaptive and selective manner. Overall, using this structure endows 
	the discriminator with the capacity to likewise model long-range dependencies, which can aid the 
	discriminator in its assessment of the image fidelity, allowing it to acquire a more holistic 
	understanding of the visual modality.
	As to the loss function, optimization and training configurations, we adopt the settings and 
	techniques used in Style-GAN2 \cite{karras2020analyzing}, including in particular style mixing, 
	stochastic variation, exponential moving average for weights, and a non-saturating logistic loss with 
	a lazy R1 regularization.
	
	To recapitulate the discussion above, the GANformer successfully unifies the GAN and Transformer 
	architectures for the task of scene generation. Compared to traditional GANs and transformers, it 
	introduces multiple key innovations:
	\begin{itemize}
		\item Compositional Latent Space with multiple variables that coordinate through attention to 
		produce the image cooperatively, in a manner that matches the inherent compositionality of 
		natural scenes.
		\item Bipartite Structure that balances between expressiveness and efficiency, modelling 
		long-range dependencies while maintaining linear computational costs.
		\item Bidirectional Interaction between the latents and the visual features, which allows the 
		refinement and interpretation of each in light of the other.
		\item Multiplicative Integration rule to impact the features' visual style more flexibly, akin to 
		StyleGAN but in contrast to the transformer network.
	\end{itemize}
	As we see in the following section, the combination of these design choices yields a strong 
	architecture that demonstrates high efficiency, improved latent space disentanglement, and 
	enhanced transparency of its generation process.
	
	\section{Implementation}
	%	This section should be structured as follows (from the Reproducibility challenge template):
	%	
	%	---
	%	
	%	Briefly describe what you did and which resources did you use. E.g. Did you use author's code, 
	%	did you re-implement parts of the pipeline, how much time did it take to produce the results, 
	%	what hardware you were using and how long it took to train/evaluate. 
	
	The code has been implemented using code and information provided by the authors.
	
	\subsection{Datasets}	
	The original paper \cite{hudson2021generative} explored the GANformer model on four datasets for 
	images and scenes: CLEVR \cite{johnson2017clevr}, LSUN-Bedrooms \cite{yu2015lsun}, Cityscapes 
	\cite{cordts2016cityscapes} and FFHQ \cite{karras2019style}. 
	
	Initially, we tried to use the Cityscapes dataset, since it is the smaller among the four: it contains 
	24998 images with 256x256 resolution. 
	However, the time to complete the training was to high on this dataset (TODO: how much?), even if 
	using computational resources like GPU.
	
	For this reason, we switch to another dataset, the Google Cartoon Set \cite{cartoonset}\footnote{	
	\url{https://google.github.io/cartoonset}}, containing 10k 2D cartoon avatar 
	images with 64x64 resolution, composed of 16 components that vary in 10 artwork attributes, 4 
	colour attributes, and 4 proportion attributes (see in Table \ref{tab:dataset}). 

	\begin{table}[htb]
		\centering
		\caption{Attributes of the Cartoon Set \url{https://google.github.io/cartoonset/download}.}
		\label{tab:dataset}
		\small
		\begin{tabularx}{\textwidth}{ll|r|X}
			&& \textbf{\# Variants} & \textbf{Description}                              \\
			\toprule
			\multirow{10}*{\textbf{Artwork}} 	&	\texttt{chin\_length}           & 3           & Length of chin 
			(below 	mouth region)      \\
			&	\texttt{eye\_angle}             & 3           & Tilt of the eye inwards or outwards      \\
			&	\texttt{eye\_lashes}            & 2           & Whether or not eyelashes are visible     \\
			&	\texttt{eye\_lid}               & 2           & Appearance of the eyelids      	\\
			&	\texttt{eyebrow\_shape}        & 14          & Shape of eyebrows        \\
			&	\texttt{eyebrow\_weight}        & 2           & Line weight of eyebrows           \\
			&	\texttt{face\_shape}            & 7           & Overall shape of the face                \\
			&	\texttt{facial\_hair}           & 15          & Type of facial hair (type 14 is no hair) \\
			&	\texttt{glasses}                & 12          & Type of glasses (type 11 is no glasses)  \\
			&	\texttt{hair}                   & 111         & Type of head hair                        \\
			\midrule
			\multirow{4}*{\textbf{Colors}} &	\texttt{eye\_color}    & 5 & Color of the eye irises           \\
			&	\texttt{face\_color}            & 11          & Color of the face skin                   \\
			&	\texttt{glasses\_color}         & 7           & Color of the glasses, if present         \\
			&	\texttt{hair\_color}        & 10     & Color of the hair, facial hair, and eyebrows      \\
			\midrule
			\multirow{4}*{\textbf{Proportions}} &	\texttt{eye\_eyebrow\_distance} & 3           & Distance 
			between the eye and eyebrows    \\
			&	\texttt{eye\_slant}             & 3           & Similar to eye\_angle, but rotates the eye and does 
			not change artwork  \\
			&	\texttt{eyebrow\_thickness}     & 4           & Vertical scaling of the eyebrows         \\
			&	\texttt{eyebrow\_width}         & 3           & Horizontal scaling of the eyebrows            \\
			\bottomrule                         
		\end{tabularx}
	\end{table}
	

	\subsection{Hyper-parameters}%TODO fixme
	%	Describe how you set the hyperparameters and what was the source for their value (e.g. paper, 
	%	code or your guess). 
	
	Note that in the code provided by the author \cite{hudson2021generative}, the hyper-parameters are 
	not the same mentioned in the article. 
	
	
	
	\subsection{Experimental setup}	
	The source code of our work is available at the following GitHub repository: 
	\url{https://github.com/GiorgiaAuroraAdorni/gansformer-reproducibility-challenge}.
	 
	 The approaches proposed in both the original paper codebase by \citet{karras2020analyzing} and 
	 by \citet{hudson2021generative} have been implemented in Python using TensorFlow 
	 \cite{tensorflow2015-whitepaper}, so, according to that, we used the same setup.
	 We created a Jupyter Notebook which runs all the experiments in Google Colaboratory, which 
	 allows us to write and execute Python in the browser. %TODO add google colab url
	
	 All the models have been trained on a Tesla P100-PCIE-16GB (GPU) provided by Google 
	 Colab Pro.
	
	\subsection{Computational requirements}%TODO fixme
	%	Provide information on computational requirements for each of your experiments. For example, 
	%   the  number of CPU/GPU hours and memory requirements. You'll need to think about this ahead 
	%	of time, and write your code in a way that captures this information so you can later add it to 
	%	this section. 
	In the original paper \cite{hudson2021generative}, they evaluate all models under comparable 
	conditions of training scheme, model size, and optimization details, implementing all the models 
	within the codebase introduced by the Style-GAN authors \cite{karras2020analyzing}. 
	All models have been trained with images of 256 × 256 resolution and for the 
	same number of training steps, roughly spanning a week on 2 NVIDIA V100 GPUs per model (or 
	equivalently 3-4 days using 4 GPUs). 
	
	Considered that we had available just one GPU and not enough time to reproduce this settings, we 
	decided to resize the images from 256×256 to 64x64 resolution.
	
	
	For the GANformer, we select k–the number of latent variables, from the range of 8–32. 
	
	All models have been trained for the same number of steps, which ranges between 5k to 15k kimg 
	training samples. 
	
	The paper present results after training 100k, 200k, 500k, 1m, 2m, 5m and 10m samples
	
	For the StyleGAN2 model we present results after training 100k (2h30m), 200k (6h), 500k, obtaining 
	good results.
	Note that the original StyleGAN2 model has been trained by its authors \cite{karras2020analyzing} 
	for up to 70k kimg samples, which is expected to take over 90 GPU-days for a single model. 
	

	For the GANformer, the authors \cite{karras2020analyzing} show impressive results, especially when 
	using duplex attention: the model manages to learn a lot faster than competing approaches, 
	generating astonishing images early in the training. This model is expected to take 4 GPU-days.
	
	However, we are not able to replicate this achievements, first because this model learns significantly 
	slower than the StyleGAN2, which is instead able to produce high-quality images in approximately 
	x-times less training steps than the GANformer.
	
	
	(in the paper they reach better results with the GANformer with 3-times less training steps than the 
	StyleGAN2, but they don't specify the time required for this training steps...)

	\section{Results}%TODO fixme
	%	In this section you should report the results of your work (e.g., the outcome of an empirical 
	%	analysis). You should be objective and support your statements with empirical evidence.
	%	
	%	Use figures, plots and tables to present your results in a nice and readable way.
	%	
	%	Refer to the Reproducibility Challenge template for hints on how to structure this in case you are 
	%	trying to replicate someone else's results.
	
	They evaluate the models along commonly used metrics such as FID, IS, and Precision \& Recall 
	scores
	
	Sample images from different points in training are based on the same sampled latent vectors, 
	thereby showing how the image evolves during the training.
	
	
	For CLEVR and Cityscapes, we present results after training to generate 100k, 200k, 500k, 1m, and 
	2m samples. For the Bedroom case, we present results after 500k, 1m, 2m, 5m and 10m generated 
	samples during training
	
	These results show how the GANformer, especially when using duplex attention, manages to learn a 
	lot faster than competing approaches, generating impressive images early in the training
	
	\section{Discussion and conclusion}%TODO
	%	Here you can express your judgments and draw your conclusions based on the  evidences 
	%	produced on the previous sections.
	%	
	%	Try to summarize the achievements of your project and its limits, suggesting (when appropriate) 
	%	possible extensions and future works.
	
	%TODO where should we put the author contributions?
	
	%%%web
	\bibliography{bibliography}
	\bibliographystyle{unsrtnat}
	\clearpage
	
\end{document}
