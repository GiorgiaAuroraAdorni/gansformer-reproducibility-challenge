\PassOptionsToPackage{square,comma,numbers,sort&compress}{natbib}

\documentclass{article}

\usepackage[preprint]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[inline]{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols

\title{ATML Report}

% The \author macro works with any number of authors.

\author{%
		Giorgia Adorni \\
		\texttt{giorgia.adorni@usi.ch} \\
		\And
		Felix Boelter\\
		\texttt{felix.boelter@usi.ch}\\
		\And
		Stefano Carlo Lambertenghi\\
		\texttt{stefano.carlo.lambertenghi@usi.ch}\\
}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Concise and self-contained description of your project, motivation and main findings.
	\end{abstract}
	
%	\section*{General notes}
%	
%	The report should be written as an article intended to present the findings of your work. Your aim 
%	should be to be clear and objective, substantiating your claims with references or 
%	empirical/theoretical evidence.
%	
%	We are well aware of the fact that carrying out machine learning experiments might be difficult and 
%	that often the final performance might be disappointing. For this reason, you will not be evaluated 
%	solely on quantitative aspect of your work, but mainly on the quality of your analysis and report.
%	
%	The length of the report should be between 4 and 8 pages (without considering references).
	
	\section{Introduction}
	This project consists in a reproducibility challenge whose objective is to investigate the 
	reliability and reproducibility of a paper accepted for publication in a top machine learning 
	conference.
	The models have been implemented using code and information provided by the authors.	
	Actually we are not participating formally to the Fall Edition of the ML Reproducibility Challenge 2021.
	
	We selected the paper ``\emph{Generative adversarial transformers}'' by 
	\citet{hudson2021generative} and verified the empirical results and claims in it by reproducing three 
	of the computational experiments performed by the authors:
	\begin{enumerate*}
		\item[(1)] the \textbf{StyleGAN2} by \citet{karras2020analyzing}, a GAN network which uses one 
		global latent style vector to modulate the features of each layer, hence to control the style of all 
		image features globally,
		\item[(2)] the \textbf{GANformer} with \textbf{Simplex Attention} by 
		\citet{hudson2021generative}, a generalisation of the StyleGAN design with \textit{k} latent 
		vectors that cooperate through attention, allowing for a spatially finer control over the generation 
		process since multiple style vectors impact different regions in the image concurrently, in 
		particular permitting communication in one direction, in the generative context – from the latents 
		to the image features, and
		\item[(3)] the \textbf{GANformer} with \textbf{Duplex Attention} by \citet{hudson2021generative}, 
		which is based on the same principles as the previous but propagating information both from 
		global latents to local image features, enabling both top-down and bottom-up reasoning to occur 
		simultaneously.
	\end{enumerate*} 
	
	The first model is used as baseline, while the remaining are the architectures introduced by the 
	authors. Thy consider the GANformer has ``a novel and efficient type of transformer'' which 
	demonstrates its strength and robustness over a range of task of visual generative modelling —  
	simulated multi-object environments (real-world indoor and out-door scenes) — achieving 
	state-of-the-art results in terms of both image quality and diversity, while benefiting from fast 
	learning and better data-efficiency. 
	
	% summary of the main results and contributions

	\section{Related works}	
	
	\subsection{Generative Adversarial Networks (GANs)}
	Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, are discriminative models 
	that learn to determine whether a sample is from the model distribution or the data distribution. 
	The architecture is composed by two main neural networks: a generator $G(z)$, which captures the 
	data distribution mapping random noise $z$ to the data space, and a discriminator $D(x)$ which 
	estimates the probability that sample $x$ is generated by $G$ or is a real sample, so it is coming 
	from the training data.

	The training procedure has two objective: the discriminator $D$ is trained to maximise the 
	probability of assigning the correct label to both training examples and samples from $G$, 
	while the generator $G$ is trained to maximise the probability of $D$ making a mistake, so it aims to 
	minimise $\log(1-D(G(z))))$.

	Combining the two objectives for $G(z)$ and $D(x)$ we get the GAN min-max game with the value 
	function $V(G,D)$:
	
	\begin{equation}
	\label{e:minmaxgame}
	\min_G \max_D V(G,D) = 
	\mathbb{E}_{x \sim p*(x)} [\log D(x)] + \mathbb E _{z \sim p_z(z)} [\log (1-D(G(z))))]
	\end{equation}
	
	
	
	\subsection{StyleGAN2}
	\cite{karras2019style} \cite{karras2020analyzing}
	
	\subsection{Transformers}
	\cite{vaswani2017attention}
	
	
	\section{Methodology}
	%	In this section you should give a description of the methodological aspects of your work, for 
	%	instance how you modified an existing method to perform a particular task or to overcome a 
	%	particular limitation. If your project is about reproducibility, here you should describe the method 
	%	presented in the original paper.
	
	\subsection{Generative Adversarial Transformers}
	
	\section{Implementation}
	%	This section should be structured as follows (from the Reproducibility challenge template):
	%	
	%	---
	%	
	%	Briefly describe what you did and which resources did you use. E.g. Did you use author's code, 
	%	did you re-implement parts of the pipeline, how much time did it take to produce the results, 
	%	what hardware you were using and how long it took to train/evaluate. 
	
	The code has been implemented using code and information provided by the authors.
	
	\subsection{Datasets}	
	The original paper \cite{hudson2021generative} explored the GANformer model on four datasets for 
	images and scenes: CLEVR \cite{johnson2017clevr}, LSUN-Bedrooms \cite{yu2015lsun}, Cityscapes 
	\cite{cordts2016cityscapes} and FFHQ \cite{karras2019style}. 
	
	Initially, we tried to use the Cityscapes dataset, since it is the smaller among the four: it contains 
	24998 images with 256x256 resolution. 
	However, the time to complete the training was to high on this dataset (TODO: how much?), even if 
	using computational resources like GPU.
	
	For this reason, we switch to another dataset, the Google Cartoon Set \cite{cartoonset}\footnote{	
	\url{https://google.github.io/cartoonset}}, containing 10k 2D cartoon avatar 
	images with 64x64 resolution, composed of 16 components that vary in 10 artwork attributes, 4 
	colour attributes, and 4 proportion attributes (see in Table \ref{tab:dataset}). 

	\begin{table}[htb]
		\centering
		\caption{Attributes of the Cartoon Set \url{https://google.github.io/cartoonset/download}.}
		\label{tab:dataset}
		\small
		\begin{tabularx}{\textwidth}{ll|r|X}
			&& \textbf{\# Variants} & \textbf{Description}                              \\
			\toprule
			\multirow{10}*{\textbf{Artwork}} 	&	\texttt{chin\_length}           & 3           & Length of chin 
			(below 	mouth region)      \\
			&	\texttt{eye\_angle}             & 3           & Tilt of the eye inwards or outwards      \\
			&	\texttt{eye\_lashes}            & 2           & Whether or not eyelashes are visible     \\
			&	\texttt{eye\_lid}               & 2           & Appearance of the eyelids      	\\
			&	\texttt{eyebrow\_shape}        & 14          & Shape of eyebrows        \\
			&	\texttt{eyebrow\_weight}        & 2           & Line weight of eyebrows           \\
			&	\texttt{face\_shape}            & 7           & Overall shape of the face                \\
			&	\texttt{facial\_hair}           & 15          & Type of facial hair (type 14 is no hair) \\
			&	\texttt{glasses}                & 12          & Type of glasses (type 11 is no glasses)  \\
			&	\texttt{hair}                   & 111         & Type of head hair                        \\
			\midrule
			\multirow{4}*{\textbf{Colors}} &	\texttt{eye\_color}    & 5 & Color of the eye irises           \\
			&	\texttt{face\_color}            & 11          & Color of the face skin                   \\
			&	\texttt{glasses\_color}         & 7           & Color of the glasses, if present         \\
			&	\texttt{hair\_color}        & 10     & Color of the hair, facial hair, and eyebrows      \\
			\midrule
			\multirow{4}*{\textbf{Proportions}} &	\texttt{eye\_eyebrow\_distance} & 3           & Distance 
			between the eye and eyebrows    \\
			&	\texttt{eye\_slant}             & 3           & Similar to eye\_angle, but rotates the eye and does 
			not change artwork  \\
			&	\texttt{eyebrow\_thickness}     & 4           & Vertical scaling of the eyebrows         \\
			&	\texttt{eyebrow\_width}         & 3           & Horizontal scaling of the eyebrows            \\
			\bottomrule                         
		\end{tabularx}
	\end{table}
	

	\subsection{Hyper-parameters}
	%	Describe how you set the hyperparameters and what was the source for their value (e.g. paper, 
	%	code or your guess). 
	
	\subsection{Experimental setup}	
	The source code of our work is available at the following GitHub repository: 
	\url{https://github.com/GiorgiaAuroraAdorni/gansformer-reproducibility-challenge}.
	 
	 The approaches proposed in both the original paper codebase by \citet{karras2020analyzing} and 
	 by \citet{hudson2021generative} have been implemented in Python using TensorFlow 
	 \cite{tensorflow2015-whitepaper}, so, according to that, we used the same setup.
	 We created a Jupyter Notebook which runs all the experiments in Google Colaboratory, which 
	 allows us to write and execute Python in the browser. %TODO add google colab url
	
	 All the models have been trained on a Tesla P100-PCIE-16GB (GPU) provided by Google 
	 Colab Pro.
	
	\subsection{Computational requirements}
	%	Provide information on computational requirements for each of your experiments. For example, 
	%   the  number of CPU/GPU hours and memory requirements. You'll need to think about this ahead 
	%	of time, and write your code in a way that captures this information so you can later add it to 
	%	this section. 
	
	stylegan2 2 hours and 30 minutes for the training 
	

	\section{Results}
	%	In this section you should report the results of your work (e.g., the outcome of an empirical 
	%	analysis). You should be objective and support your statements with empirical evidence.
	%	
	%	Use figures, plots and tables to present your results in a nice and readable way.
	%	
	%	Refer to the Reproducibility Challenge template for hints on how to structure this in case you are 
	%	trying to replicate someone else's results.
	
	\section{Discussion and conclusion}
	%	Here you can express your judgments and draw your conclusions based on the  evidences 
	%	produced on the previous sections.
	%	
	%	Try to summarize the achievements of your project and its limits, suggesting (when appropriate) 
	%	possible extensions and future works.
	
	
	
	%%%%
	\nocite{*} %TODO remove
	\bibliography{bibliography}
	\bibliographystyle{unsrtnat}
	\clearpage
	
\end{document}
